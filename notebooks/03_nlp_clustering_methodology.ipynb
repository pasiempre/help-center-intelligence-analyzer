{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Notebook 03: NLP Clustering Methodology\n",
    "\n",
    "**Author:** Hector Carbajal  \n",
    "**Version:** 1.0  \n",
    "**Last Updated:** 2026-02\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook provides **technical documentation** of the NLP clustering pipeline used to group macros by topic. It demonstrates:\n",
    "\n",
    "1. **TF-IDF Vectorization**: How macro text is transformed into numerical features\n",
    "2. **Cluster Selection**: Elbow method and silhouette analysis for choosing K\n",
    "3. **Cluster Quality**: Visual proof of separation using PCA\n",
    "4. **Similarity Analysis**: Detecting redundant macro pairs\n",
    "\n",
    "## Inputs\n",
    "- `data/processed/macro_scores.csv` - Macro effectiveness scores with text\n",
    "- `models/tfidf_vectorizer.pkl` - Fitted TF-IDF vectorizer\n",
    "- `models/kmeans_model.pkl` - Fitted KMeans model\n",
    "\n",
    "## Key Findings\n",
    "- K=12 clusters selected based on elbow + silhouette analysis\n",
    "- Mean silhouette score: ~0.15-0.25 (typical for text data)\n",
    "- 3 clusters identified as consolidation candidates\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from src.config import (\n",
    "    MACRO_SCORES_FILE, MACRO_CLUSTERS_FILE, CLUSTER_SUMMARY_FILE,\n",
    "    TFIDF_VECTORIZER_FILE, KMEANS_MODEL_FILE,\n",
    "    NUM_CLUSTERS, TFIDF_MAX_FEATURES, TFIDF_NGRAM_RANGE\n",
    ")\n",
    "from src.utils import clean_text, remove_boilerplate\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Setup complete\")\n",
    "print(f\"Configuration: K={NUM_CLUSTERS}, TF-IDF features={TFIDF_MAX_FEATURES}, ngrams={TFIDF_NGRAM_RANGE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vectorization-header",
   "metadata": {},
   "source": [
    "## 1. TF-IDF Vectorization\n",
    "\n",
    "We use **Term Frequency-Inverse Document Frequency (TF-IDF)** to convert macro text into numerical vectors.\n",
    "\n",
    "**Parameters:**\n",
    "- `max_features=500`: Limit vocabulary to top 500 terms\n",
    "- `ngram_range=(1,2)`: Include unigrams and bigrams\n",
    "- `min_df=2`: Term must appear in at least 2 documents\n",
    "- `max_df=0.8`: Ignore terms in >80% of documents (too common)\n",
    "\n",
    "**Why TF-IDF over embeddings?**\n",
    "- Interpretable: We can see which terms define each cluster\n",
    "- Fast: No GPU required, runs in seconds\n",
    "- Sufficient: For macro text (short, domain-specific), TF-IDF performs well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vectorization-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load macro data\n",
    "macro_scores = pd.read_csv(MACRO_SCORES_FILE)\n",
    "print(f\"Loaded {len(macro_scores)} macros\")\n",
    "\n",
    "# Prepare text: combine name + category + body\n",
    "macro_scores['combined_text'] = (\n",
    "    macro_scores['macro_name'].fillna('') + ' ' +\n",
    "    macro_scores['category'].fillna('') + ' ' +\n",
    "    macro_scores['macro_body'].fillna('')\n",
    ")\n",
    "macro_scores['cleaned_text'] = macro_scores['combined_text'].apply(\n",
    "    lambda x: remove_boilerplate(clean_text(x))\n",
    ")\n",
    "\n",
    "# Vectorize\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=TFIDF_MAX_FEATURES,\n",
    "    ngram_range=TFIDF_NGRAM_RANGE,\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    stop_words='english'\n",
    ")\n",
    "tfidf_matrix = vectorizer.fit_transform(macro_scores['cleaned_text'])\n",
    "\n",
    "print(f\"\\nTF-IDF Matrix Shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "print(f\"\\nTop 20 terms by document frequency:\")\n",
    "\n",
    "# Show top terms\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "term_freq = np.asarray(tfidf_matrix.sum(axis=0)).flatten()\n",
    "top_indices = term_freq.argsort()[-20:][::-1]\n",
    "for idx in top_indices:\n",
    "    print(f\"  {feature_names[idx]}: {term_freq[idx]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elbow-header",
   "metadata": {},
   "source": [
    "## 2. Cluster Selection: Elbow Method\n",
    "\n",
    "We use the **elbow method** to identify the optimal number of clusters. The \"elbow\" is where adding more clusters yields diminishing returns in reducing inertia (within-cluster variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elbow-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow analysis\n",
    "K_range = range(4, 20)\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "\n",
    "X = tfidf_matrix.toarray()\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouettes.append(silhouette_score(X, labels))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elbow plot\n",
    "axes[0].plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].axvline(x=NUM_CLUSTERS, color='red', linestyle='--', label=f'Selected K={NUM_CLUSTERS}')\n",
    "axes[0].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "axes[0].set_ylabel('Inertia (Within-cluster variance)', fontsize=12)\n",
    "axes[0].set_title('Elbow Method for Optimal K', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette plot\n",
    "axes[1].plot(K_range, silhouettes, 'go-', linewidth=2, markersize=8)\n",
    "axes[1].axvline(x=NUM_CLUSTERS, color='red', linestyle='--', label=f'Selected K={NUM_CLUSTERS}')\n",
    "axes[1].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "axes[1].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[1].set_title('Silhouette Score by K', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSelected K={NUM_CLUSTERS}\")\n",
    "print(f\"Silhouette score at K={NUM_CLUSTERS}: {silhouettes[NUM_CLUSTERS-4]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-header",
   "metadata": {},
   "source": [
    "## 3. Cluster Quality: PCA Visualization\n",
    "\n",
    "To visualize cluster separation, we reduce the 500-dimensional TF-IDF vectors to 2D using **Principal Component Analysis (PCA)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final clustering with selected K\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X)\n",
    "macro_scores['cluster_id'] = labels\n",
    "\n",
    "# PCA reduction\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Cluster centers in PCA space\n",
    "centers_pca = pca.transform(kmeans.cluster_centers_)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    X_pca[:, 0], X_pca[:, 1],\n",
    "    c=labels, cmap='tab20', alpha=0.7, s=100, edgecolors='white', linewidth=0.5\n",
    ")\n",
    "\n",
    "# Plot centers\n",
    "ax.scatter(\n",
    "    centers_pca[:, 0], centers_pca[:, 1],\n",
    "    c='black', marker='X', s=300, edgecolors='white', linewidth=2,\n",
    "    label='Cluster Centers'\n",
    ")\n",
    "\n",
    "# Annotate centers\n",
    "for i, (x, y) in enumerate(centers_pca):\n",
    "    ax.annotate(f'C{i}', (x, y), fontsize=10, fontweight='bold',\n",
    "                ha='center', va='bottom', color='black')\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=12)\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=12)\n",
    "ax.set_title('Macro Clusters in PCA Space', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter, ax=ax, label='Cluster ID')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary stats\n",
    "final_silhouette = silhouette_score(X, labels)\n",
    "print(f\"\\nCluster Quality Metrics:\")\n",
    "print(f\"  Silhouette Score: {final_silhouette:.3f}\")\n",
    "print(f\"  Inertia: {kmeans.inertia_:.2f}\")\n",
    "print(f\"  PCA Variance Explained: {pca.explained_variance_ratio_.sum():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silhouette-header",
   "metadata": {},
   "source": [
    "## 4. Silhouette Analysis by Cluster\n",
    "\n",
    "The silhouette plot shows how well each sample fits within its cluster. Values close to 1 indicate good clustering; values near 0 indicate boundary samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silhouette-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute silhouette values per sample\n",
    "sample_silhouettes = silhouette_samples(X, labels)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "y_lower = 10\n",
    "for i in range(NUM_CLUSTERS):\n",
    "    cluster_silhouettes = sample_silhouettes[labels == i]\n",
    "    cluster_silhouettes.sort()\n",
    "    \n",
    "    cluster_size = len(cluster_silhouettes)\n",
    "    y_upper = y_lower + cluster_size\n",
    "    \n",
    "    color = plt.cm.tab20(i / NUM_CLUSTERS)\n",
    "    ax.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouettes,\n",
    "                     facecolor=color, edgecolor=color, alpha=0.7)\n",
    "    ax.text(-0.05, y_lower + 0.5 * cluster_size, f'C{i}', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "ax.axvline(x=final_silhouette, color='red', linestyle='--', \n",
    "           label=f'Mean: {final_silhouette:.3f}')\n",
    "ax.set_xlabel('Silhouette Coefficient', fontsize=12)\n",
    "ax.set_ylabel('Cluster', fontsize=12)\n",
    "ax.set_title('Silhouette Plot for K-Means Clustering', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_xlim([-0.2, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-cluster stats\n",
    "print(\"\\nPer-Cluster Silhouette Scores:\")\n",
    "for i in range(NUM_CLUSTERS):\n",
    "    cluster_sil = sample_silhouettes[labels == i].mean()\n",
    "    cluster_size = (labels == i).sum()\n",
    "    print(f\"  Cluster {i}: {cluster_sil:.3f} (n={cluster_size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labels-header",
   "metadata": {},
   "source": [
    "## 5. Cluster Labels & Top Keywords\n",
    "\n",
    "Each cluster is labeled based on its top TF-IDF terms, making the clusters interpretable for business stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labels-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cluster labels from top keywords\n",
    "cluster_labels = {}\n",
    "print(\"Cluster Labels (Top 5 Keywords):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for cluster_id in range(NUM_CLUSTERS):\n",
    "    cluster_mask = labels == cluster_id\n",
    "    cluster_vectors = X[cluster_mask]\n",
    "    \n",
    "    # Mean vector for cluster\n",
    "    mean_vector = cluster_vectors.mean(axis=0)\n",
    "    \n",
    "    # Top 5 terms\n",
    "    top_indices = mean_vector.argsort()[-5:][::-1]\n",
    "    top_terms = [feature_names[i] for i in top_indices]\n",
    "    \n",
    "    # Create label from top 3\n",
    "    label = ' / '.join(top_terms[:3]).title()\n",
    "    cluster_labels[cluster_id] = label\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id}: {label}\")\n",
    "    print(f\"  Size: {cluster_mask.sum()} macros\")\n",
    "    print(f\"  Keywords: {', '.join(top_terms)}\")\n",
    "\n",
    "macro_scores['cluster_label'] = macro_scores['cluster_id'].map(cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similarity-header",
   "metadata": {},
   "source": [
    "## 6. Similarity Analysis\n",
    "\n",
    "Beyond clustering, we compute **pairwise cosine similarity** to detect redundant macro pairs that may be candidates for consolidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similarity-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrix\n",
    "similarity_matrix = cosine_similarity(X)\n",
    "\n",
    "# Get upper triangle (excluding diagonal)\n",
    "upper_tri = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]\n",
    "\n",
    "# Distribution plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(upper_tri, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(x=0.8, color='red', linestyle='--', label='Redundancy threshold (0.8)')\n",
    "axes[0].set_xlabel('Cosine Similarity', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Pairwise Similarity Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Heatmap (sampled for visibility)\n",
    "sample_size = min(50, len(macro_scores))\n",
    "sample_idx = np.random.choice(len(macro_scores), sample_size, replace=False)\n",
    "sample_sim = similarity_matrix[np.ix_(sample_idx, sample_idx)]\n",
    "\n",
    "sns.heatmap(sample_sim, cmap='YlOrRd', ax=axes[1], \n",
    "            xticklabels=False, yticklabels=False,\n",
    "            cbar_kws={'label': 'Similarity'})\n",
    "axes[1].set_title(f'Similarity Heatmap (Sample of {sample_size})', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Stats\n",
    "print(\"\\nSimilarity Statistics:\")\n",
    "print(f\"  Mean similarity: {upper_tri.mean():.3f}\")\n",
    "print(f\"  Median similarity: {np.median(upper_tri):.3f}\")\n",
    "print(f\"  Max similarity: {upper_tri.max():.3f}\")\n",
    "print(f\"  Pairs above 0.8 (redundant): {(upper_tri >= 0.8).sum()}\")\n",
    "print(f\"  Pairs above 0.6 (similar): {(upper_tri >= 0.6).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "findings-header",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "**Clustering Methodology Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "findings-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"NLP CLUSTERING METHODOLOGY SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n1. VECTORIZATION:\")\n",
    "print(f\"   Method: TF-IDF (Term Frequency-Inverse Document Frequency)\")\n",
    "print(f\"   Features: {TFIDF_MAX_FEATURES} terms, ngrams={TFIDF_NGRAM_RANGE}\")\n",
    "print(f\"   Rationale: Interpretable, fast, effective for short domain-specific text\")\n",
    "\n",
    "print(f\"\\n2. CLUSTER SELECTION:\")\n",
    "print(f\"   Method: Elbow + Silhouette analysis\")\n",
    "print(f\"   Selected K: {NUM_CLUSTERS} clusters\")\n",
    "print(f\"   Silhouette Score: {final_silhouette:.3f}\")\n",
    "print(f\"   Note: Scores of 0.15-0.25 are typical for text clustering\")\n",
    "\n",
    "print(f\"\\n3. CLUSTER QUALITY:\")\n",
    "print(f\"   PCA variance explained: {pca.explained_variance_ratio_.sum():.1%}\")\n",
    "print(f\"   Clusters show visual separation in PCA space\")\n",
    "print(f\"   Each cluster has interpretable top keywords\")\n",
    "\n",
    "print(f\"\\n4. SIMILARITY ANALYSIS:\")\n",
    "print(f\"   {(upper_tri >= 0.8).sum()} macro pairs have >80% similarity (consolidation candidates)\")\n",
    "print(f\"   Mean pairwise similarity: {upper_tri.mean():.3f}\")\n",
    "\n",
    "print(f\"\\n5. LIMITATIONS:\")\n",
    "print(f\"   - TF-IDF doesn't capture semantic meaning (e.g., synonyms)\")\n",
    "print(f\"   - Future enhancement: sentence-transformers for richer embeddings\")\n",
    "print(f\"   - Cluster quality depends on macro text quality\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
